{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11fcac23-c090-48ef-8e7a-6ee148e5ad9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanishqa/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Column1       Dates                                              Links  \\\n",
      "0          2023-04-25  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "1       1  2023-01-24  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "2       1  2023-10-29  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "3       8  2023-07-25  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "4       2  2023-04-27  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "\n",
      "                                                text     label  \n",
      "0  Bro bro brosuf bitch as mother fucking regard ...  0.010114  \n",
      "1  FUCKING MORONS!!! MSFT Misses and it goes up. ...   0.01012  \n",
      "2  Seriously I don’t know why any sane company wo...  0.010152  \n",
      "3  Where is $30k $MSFT calls guy? I want kick his...  0.010177  \n",
      "4  Ugh I’m a fucking dumbass\\n\\n50 aapl 170 calls...  0.010268  \n"
     ]
    }
   ],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "json_keyfile = \"/Users/tanishqa/Downloads/vader-sentiment-analysis-e6711036e61c.json\"\n",
    "\n",
    "scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(json_keyfile, scope)\n",
    "\n",
    "client = gspread.authorize(creds)\n",
    "sheet = client.open(\"Copy of Spreadsheet Data w/ Sentiment Predictions\").sheet1\n",
    "data = sheet.get_all_records()\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.head())  # Show first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe9a126-ab2b-4cf3-8d30-7c72f7476de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Column1', 'Dates', 'Links', 'text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6e1acb-66d3-41d6-b054-0564aa5fb835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Column1       Dates                                              Links  \\\n",
      "18047       1  2023-02-01  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "22994       3  2023-11-20  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "12403       2  2023-05-01  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "16658       4  2023-01-02  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "20689       1  2023-02-07  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "\n",
      "                                                    text     label  \n",
      "18047  Hess not trading, he’s just holding long term ...  0.576233  \n",
      "22994               msft is going to moon on market open  0.844681  \n",
      "12403  But it is profitable. Only nvidia stock is exp...  0.492164  \n",
      "16658                                               MSFT  0.547687  \n",
      "20689  &gt;MICROSOFT'S AI POWERED EDGE CAN ANALAYZE F...  0.683417  \n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "df_tokenized = df.apply(lambda row: tokenizer(row['text'], padding=\"max_length\", truncation=True), axis=1)\n",
    "\n",
    "#Split training data into 80% train 20% test\n",
    "train_data, test_data = train_test_split(df, test_size=0.2)\n",
    "\n",
    "\n",
    "print(train_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26669a47-e049-420d-93dd-a0fc2a1bfeaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m test_encodings \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;28mlist\u001b[39m(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]), truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convert the labels (sentiment) to PyTorch tensors\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with your sentiment label column\u001b[39;00m\n\u001b[1;32m      9\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_column\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)  \u001b[38;5;66;03m# Replace with your sentiment label column\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Convert tokenized data into PyTorch tensors\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(list(train_data['text']), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(test_data['text']), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "\n",
    "train_labels = torch.tensor(train_data['label'].values)  # Replace with your sentiment label column\n",
    "test_labels = torch.tensor(test_data['sentiment_column'].values)  # Replace with your sentiment label column\n",
    "\n",
    "\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3fcb86-e772-4496-a3d5-13cdd21afcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5762327015 0.8446812332 0.4921642989 ... 0.07301665284 0.5391993523\n",
      " 0.01821408095]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_data['label'].unique())  # List unique values to see if anything is not numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1739035-ad1a-400f-81c5-f55b8f3edfc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert to PyTorch tensors with float data type since the labels are float values\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "\n",
    "train_labels = torch.tensor(train_data['label'].values, dtype=torch.float)\n",
    "test_labels = torch.tensor(test_data['label'].values, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea392273-43d7-46e4-978f-3e16c09525d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data['label'] = pd.to_numeric(train_data['label'], errors='coerce')\n",
    "test_data['label'] = pd.to_numeric(test_data['label'], errors='coerce')\n",
    "\n",
    "\n",
    "train_data = train_data.dropna(subset=['label'])\n",
    "test_data = test_data.dropna(subset=['label'])\n",
    "\n",
    "train_labels = torch.tensor(train_data['label'].values, dtype=torch.float)\n",
    "test_labels = torch.tensor(test_data['label'].values, dtype=torch.float)\n",
    "\n",
    "\n",
    "print(train_labels.dtype)\n",
    "print(test_labels.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec5c538b-5735-4670-8065-f8bce66aaa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18047    0.576233\n",
      "22994    0.844681\n",
      "12403    0.492164\n",
      "16658    0.547687\n",
      "20689    0.683417\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train_data['label'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b224cae0-c9d0-4b9d-b3d7-0b955380003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(list(train_data['text']), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(test_data['text']), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fc87ea8-5a7a-4e5b-92f7-67c02dda26a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t4/hm0qvm6d4y5cnzj9d_ll99lc0000gn/T/ipykernel_58377/3621506204.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_inputs = torch.tensor(train_encodings['input_ids'])\n",
      "/var/folders/t4/hm0qvm6d4y5cnzj9d_ll99lc0000gn/T/ipykernel_58377/3621506204.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_inputs = torch.tensor(test_encodings['input_ids'])\n"
     ]
    }
   ],
   "source": [
    "# Convert tokenized data into PyTorch tensors\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "399eb989-dfa7-4296-a080-8e6ffdf9e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, inputs, labels, attention_mask):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(train_encodings['input_ids'], train_labels, train_encodings['attention_mask'])\n",
    "test_dataset = SentimentDataset(test_encodings['input_ids'], test_labels, test_encodings['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c62ce208-4d09-4dd0-abb1-1caa4ede8767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "972035ae-30ac-4cff-9420-c0daa87c47cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# Initialize the DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=1)  # num_labels=1 for regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb85dd08-c0ba-46dd-b701-54bae4206048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")  # Force CPU usage\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f8270-294c-4b24-a24f-bb2589608471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1...\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "# Define loss function (if not already included in the model)\n",
    "loss_fn = torch.nn.MSELoss()  # Use MSELoss since labels are continuous values (float)\n",
    "\n",
    "# Move model to the correct device (CPU/GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Example of a simple training loop with attention_mask\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Starting epoch {epoch+1}...\")  # Debug: Print when each epoch starts\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get inputs, attention_mask, and labels from the batch\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels'] \n",
    "# Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss for this epoch\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd0426-9788-42a8-931b-2fff9a94a868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
